{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Assessment \n",
    "\n",
    "**For all the datasets mentioned in questions visit - https://github.com/innomaticsresearchlab/Datasets**\n",
    "\n",
    "# Statistical Learning \n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "**Q. This exercise relates to the College data set, which can be found in the file College.csv mentioned in the above link. It contains a number of variables for 777 different universities and colleges in the US.**\n",
    "\n",
    "(a) Use the read_csv() function to read the data. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\n",
    "\n",
    "(b) Write the assumptions to treat the first column which is just the name of each university.\n",
    "\n",
    "(c) \n",
    "    \n",
    "    (i) Describe the variables in the dataset.\n",
    "\n",
    "    (ii) Draw scatterplots for all columns.\n",
    "    \n",
    "    (iii) Use boxplot and produce side by side boxplots of Outstate versus Private.\n",
    "    \n",
    "    (iv) Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. \n",
    "    \n",
    "    Now find how many elite universities there are. Plot sis by side box plots of Outstate versus Elite.\n",
    "     \n",
    "    (v) Use any function to produce some histograms with differing numbers of bins for a few of the quantitative variables. \n",
    "    \n",
    "    (vi) Continue exploring the data, and provide a brief summary of what you discover.\n",
    "    \n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "**Q. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.**\n",
    "\n",
    "(a) Which of the predictors are quantitative, and which are qualitative?\n",
    "\n",
    "(b) What is the range of each quantitative predictor?\n",
    "\n",
    "(c) What is the mean and standard deviation of each quantitative predictor?\n",
    "\n",
    "(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n",
    "\n",
    "(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment\n",
    "on your findings.\n",
    "\n",
    "(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "**This exercise involves the Boston housing data set.**\n",
    "\n",
    "(a) How many rows are in this data set? How many columns? What do the rows and columns represent?\n",
    "\n",
    "(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\n",
    "\n",
    "(c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.\n",
    "\n",
    "(d) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n",
    "\n",
    "(e) How many of the suburbs in this data set bound the Charles river?\n",
    "\n",
    "(f) What is the median pupil-teacher ratio among the towns in this data set?\n",
    "\n",
    "(g) Which suburb of Boston has lowest median value of owneroccupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings.\n",
    "\n",
    "(h) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "**Q. This question involves the use of simple linear regression on the Auto\n",
    "data set.**\n",
    "\n",
    "(a) Use the ols() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.\n",
    "For example:\n",
    "\n",
    "    i. Is there a relationship between the predictor and the response?\n",
    "    \n",
    "    ii. How strong is the relationship between the predictor and the response?\n",
    "    \n",
    "    iii. Is the relationship between the predictor and the response positive or negative?\n",
    "    \n",
    "    iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?\n",
    "    \n",
    "   \n",
    "(b) Plot the response and the predictor. Use the respective function to display the least squares regression line.\n",
    "\n",
    "(c) Use any required plot function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Doubt\n",
    "\n",
    "**Q. This question involves the use of multiple linear regression on the Auto data set.**\n",
    "\n",
    "(a) Produce a scatterplot matrix which includes all of the variables in the data set.\n",
    "\n",
    "(b) Compute the matrix of correlations between the variables using the function corr().\n",
    "\n",
    "(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results.\n",
    "Comment on the output. For instance:\n",
    "\n",
    "    i. Is there a relationship between the predictors and the response?\n",
    "\n",
    "    ii. Which predictors appear to have a statistically significant relationship to the response?\n",
    "    \n",
    "    iii. What does the coefficient for the year variable suggest?\n",
    "\n",
    "(d) Use any required plot function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n",
    "\n",
    "(e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n",
    "\n",
    "(f) Try a few different transformations of the variables, such as log(X),√X, X2. Comment on your findings.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "**Q. This question should be answered using the Carseats data set.**\n",
    "\n",
    "(a) Fit a multiple regression model to predict Sales using Price, Urban, and US.\n",
    "\n",
    "(b) Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!\n",
    "\n",
    "(c) Write out the model in equation form, being careful to handle the qualitative variables properly.\n",
    "\n",
    "(d) For which of the predictors can you reject the null hypothesis $H_0 : β_j = 0$?\n",
    "\n",
    "(e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n",
    "\n",
    "(f) How well do the models in (a) and (e) fit the data?\n",
    "\n",
    "(g) Using the model from (e), obtain 95% confidence intervals for the coefficient(s).\n",
    "\n",
    "(h) Is there evidence of outliers or high leverage observations in the model from (e)?\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Q. In this problem we will investigate the t-statistic for the null hypothesis $H_0 : β = 0$ in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.\n",
    "\n",
    "$np.random.seed(1)$\n",
    "\n",
    "$x = np.random.randn(100)$\n",
    "\n",
    "$y = 2*x + np.random.randn(100)$\n",
    "\n",
    "(a) Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate $\\hat β$, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0 : β = 0$. Comment on these results.\n",
    "\n",
    "(b) Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis $H_0 : β = 0$. Comment on these results.\n",
    "\n",
    "(c) What is the relationship between the results obtained in (a) and (b)?\n",
    "\n",
    "(d) For the regression of Y onto X without an intercept, the tstatistic for H0 : β = 0 takes the form \n",
    "\n",
    "$\\hat β/SE(\\hat β), \n",
    "\n",
    "where \\hat β$ is given by\n",
    "\n",
    "# $\\hat \\beta = \\frac {\\sum_{i=1} ^n x_iy_i}{\\sum_{i^`=1} ^n x_i} $\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
